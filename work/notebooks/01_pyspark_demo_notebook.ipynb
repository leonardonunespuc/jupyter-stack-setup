{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Demo Notebook\n",
    "## Demo\n",
    "1. Run PostgreSQL Script\n",
    "2. Load PostgreSQL Data\n",
    "3. Create New Record\n",
    "4. Write New Record to PostgreSQL Table\n",
    "5. Load CSV Data File\n",
    "6. Write Data to PostgreSQL\n",
    "7. Analyze Data with Spark SQL\n",
    "8. Graph Data with BokehJS\n",
    "9. Read and Write Data to Parquet Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PostgreSQL Script\n",
    "Run the PostgreSQL sql script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../scripts/03_load_sql.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark_demo_app') \\\n",
    "    .config('spark.driver.extraClassPath', '../driver/postgresql-42.2.5.jar') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PostgreSQL Data\n",
    "Load the PostgreSQL 'bakery_basket' table's contents into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    'driver': 'org.postgresql.Driver',\n",
    "    'url': 'jdbc:postgresql://postgres:5432/demo',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres1234',\n",
    "    'dbtable': 'bakery_basket',\n",
    "}\n",
    "\n",
    "df1 = spark.read \\\n",
    "    .format('jdbc') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('url', properties['url']) \\\n",
    "    .option('user', properties['user']) \\\n",
    "    .option('password', properties['password']) \\\n",
    "    .option('dbtable', properties['dbtable']) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df1.show(10)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Record\n",
    "Create a new bakery record and load into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('2016-10-30', '10:13:27', 2, 'Pastry')]\n",
    "\n",
    "bakery_schema = StructType([\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('time', StringType(), True),\n",
    "    StructField('transaction', IntegerType(), True),\n",
    "    StructField('item', StringType(), True)\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data, bakery_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write New Record to PostgreSQL Table\n",
    "Append the contents of the DataFrame to the PostgreSQL 'bakery_basket' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write \\\n",
    "    .format('jdbc') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('url', properties['url']) \\\n",
    "    .option('user', properties['user']) \\\n",
    "    .option('password', properties['password']) \\\n",
    "    .option('dbtable', properties['dbtable']) \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(10)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV File Data\n",
    "Load the Kaggle dataset from the CSV file, containing ~21K records, into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"../datasets/BreadBasket_DMS.csv\", schema=bakery_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(10)\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data to PostgreSQL\n",
    "Append the contents of the DataFrame to the PostgreSQL 'bakery_basket' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write \\\n",
    "    .format('jdbc') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('url', properties['url']) \\\n",
    "    .option('user', properties['user']) \\\n",
    "    .option('password', properties['password']) \\\n",
    "    .option('dbtable', properties['dbtable']) \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(10)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Data with Spark SQL\n",
    "Analyze the DataFrame's bakery data using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"bakery_table\")\n",
    "df4 = spark.sql(\"SELECT * FROM bakery_table \" +\n",
    "                \"ORDER BY transaction, date, time\")\n",
    "df4.show(10)\n",
    "df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = spark.sql(\"SELECT COUNT(DISTINCT item) AS item_count FROM bakery_table\")\n",
    "df5.show()\n",
    "\n",
    "df5 = spark.sql(\"SELECT item, count(*) as count \" +\n",
    "                \"FROM bakery_table \" +\n",
    "                \"WHERE item NOT LIKE 'NONE' \" +\n",
    "                \"GROUP BY item ORDER BY count DESC \" +\n",
    "                \"LIMIT 10\")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Data with BokehJS\n",
    "Create a vertical bar chart displaying DataFrame data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Paired12\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(data=df5.toPandas())\n",
    "\n",
    "tooltips = [('item', '@item'), ('count', '@{count}{,}')]\n",
    "\n",
    "items = source.data['item'].tolist()\n",
    "color_map = factor_cmap(field_name='item', palette=Paired12, factors=items)\n",
    "plot = figure(x_range=items, plot_width=750, plot_height=375, min_border=0, tooltips=tooltips)\n",
    "plot.vbar(x='item', bottom=0, top='count', source=source, width=0.9, fill_color=color_map)\n",
    "plot.title.text = 'Top 10 Bakery Items'\n",
    "plot.xaxis.axis_label = 'Bakery Items'\n",
    "plot.yaxis.axis_label = 'Total Items Sold'\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = spark.sql(\"SELECT CONCAT(date,' ',time) as timestamp, transaction, item \" +\n",
    "                \"FROM bakery_table \" +\n",
    "                \"WHERE item NOT LIKE 'NONE'\" +\n",
    "                \"ORDER BY transaction\"\n",
    "               )\n",
    "df6.show(10)\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df6.withColumn('timestamp', to_timestamp(df6.timestamp, 'yyyy-MM-dd HH:mm:ss'))\n",
    "df7.printSchema()\n",
    "df7.show(10)\n",
    "df7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.createOrReplaceTempView(\"bakery_table\")\n",
    "df8 = spark.sql(\"SELECT DISTINCT * \" +\n",
    "                \"FROM bakery_table \" +\n",
    "                \"WHERE item NOT LIKE 'NONE'\" +\n",
    "                \"ORDER BY transaction DESC\"\n",
    "                )\n",
    "df8.show(10)\n",
    "df8.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Write Data to Parquet Format\n",
    "Read and write DataFrame data to Parquet format files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.write.parquet('output/bakery_parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = spark.read.parquet('output/bakery_parquet')\n",
    "df9.show(10)\n",
    "df9.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
